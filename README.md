# XOR Neural Network Library

[![C++17](https://img.shields.io/badge/C%2B%2B-17-blue.svg)](https://isocpp.org/) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

A *header-only* C++ library for building and training simple feedforward neural networks.
This example project demonstrates a 2-layer network solving the XOR problem and exporting results for visualization.

---

## ğŸ“‚ Repository Structure

```plaintext
net/
â”œâ”€â”€ include/            # Header-only library files
â”‚   â”œâ”€â”€ Activation.hpp  # Activation functions (tanh, sigmoid)
â”‚   â”œâ”€â”€ Loss.hpp        # Loss functions & gradients (binary cross-entropy)
â”‚   â”œâ”€â”€ Initialisers.hpp# Weight initialisers (Xavier, reproducible)
â”‚   â”œâ”€â”€ Layer.hpp       # Abstract Layer interface
â”‚   â”œâ”€â”€ DenseLayer.hpp  # Fully-connected layer implementation
â”‚   â””â”€â”€ Network.hpp     # Model orchestration (forward/backward)
â””â”€â”€ app/                # Example application
    â””â”€â”€ main.cpp        # Train XOR network & generate CSV

Makefile               # Build rules
README.md              # This document
```

---

## ğŸ”§ Prerequisites

* **Compiler**: Any C++17-compatible (e.g., `g++`, `clang++`)
* **Build tool**: GNU Make (optional)

---

## âš™ï¸ Building & Running

### Using Make (recommended)

```bash
# Build executable
make

# Train & run
make run
```

### Manual build

```bash
g++ -std=c++17 -O2 -Iinclude app/main.cpp -o xor_net
./xor_net
```

On success, youâ€™ll see training logs:

```
Epoch 1  loss 0.6931
Epoch 1000  loss 0.6930
â€¦
Testing XOR:
(0,0) -> 0.01
(0,1) -> 0.98
â€¦
```

and a file `xor_grid.csv` with `x,y,prob` for contour plots.

---

## ğŸš€ Usage in Your Project

Add `include/` to your compilerâ€™s include path:

```cpp
#include "Activation.hpp"
#include "Loss.hpp"
#include "Initialisers.hpp"
#include "DenseLayer.hpp"
#include "Network.hpp"
```

Then construct and train:

```cpp
Network net;
// Two-layer network: 2â†’3â†’1
auto &l1 = net.add<DenseLayer>(2, 3, act::tanh, act::tanh_prime);
auto &l2 = net.add<DenseLayer>(3, 1, act::sigmoid, act::sigmoid_prime);

// Reproducible init\ nxavier(l1.W, l1.b, 2, 3, 1234);
xavier(l2.W, l2.b, 3, 1, 1234);

// Training loop
for (â€¦)
  net.forward(input);
  net.backward(input, loss::bce_grad(output, target), lr);
```

---

## ğŸ› ï¸ Extending with New Layers

1. Create a header (e.g. `Conv2DLayer.hpp`) in `include/`.
2. Derive from `Layer` and implement:

   * `forward`
   * `backward`
   * `backward_output`
   * `weights()` accessor
3. Register your layer:

```cpp
net.add<Conv2DLayer>(in_ch, out_ch, kernel_size, stride, padding);
```

No changes to core library neededâ€”`Network` handles any `Layer`.

---

## ğŸ“Š Visualization

Use the generated `xor_grid.csv` with Python/Matplotlib or your favorite tool:

```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('xor_grid.csv')
plt.tricontourf(df.x, df.y, df.prob, levels=50)
plt.show()
```

---

## ğŸ“ License

Distributed under the MIT License. See [LICENSE](LICENSE) for details.
